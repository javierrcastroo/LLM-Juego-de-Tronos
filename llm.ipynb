{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juego de Tronos (Libro 1): QA + Imagen (Notebook independiente)\n",
    "\n",
    "Este notebook es **autocontenido**: no depende de archivos `.py` externos.\n",
    "Modelos:\n",
    "- QA/Planner: `Qwen/Qwen3-30B-A3B-Thinking-2507-FP8`\n",
    "- Imagen: `stabilityai/stable-diffusion-3.5-large`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \"pandas==2.2.2\"\n",
    "!pip -q install -U pyarrow beautifulsoup4 lxml faiss-cpu sentence-transformers transformers accelerate diffusers safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "EPUB_PATH = '/content/drive/MyDrive/juego_de_tronos.epub'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "497636f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from typing import Optional\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "QWEN_MODEL_ID = 'Qwen/Qwen3-30B-A3B-Thinking-2507'\n",
    "EMBED_MODEL_ID = 'BAAI/bge-m3'\n",
    "RERANKER_MODEL_ID = 'BAAI/bge-reranker-large'\n",
    "SD3_MODEL_ID = 'stabilityai/stable-diffusion-3-medium-diffusers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\r\\n?\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_title_and_pov(text: str) -> tuple[Optional[str], Optional[str]]:\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "    for line in lines[:20]:\n",
    "        if re.match(r\"^[A-ZÁÉÍÓÚÑÜ]+.*\\(\\d+\\)$\", line):\n",
    "            return line, line.split(\"(\")[0].strip()\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def list_xhtml_text_files(zf: zipfile.ZipFile) -> list[str]:\n",
    "    candidates = [f for f in zf.namelist() if f.lower().endswith((\".xhtml\", \".html\"))]\n",
    "    preferred = [f for f in candidates if '/text/' in f.lower() or '/texto/' in f.lower()]\n",
    "    return sorted(preferred if preferred else candidates)\n",
    "\n",
    "\n",
    "def extract_chapters(epub_path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(epub_path):\n",
    "        raise FileNotFoundError(f'No existe EPUB_PATH: {epub_path}')\n",
    "\n",
    "    chapters = []\n",
    "    with zipfile.ZipFile(epub_path, 'r') as zf:\n",
    "        for file_name in list_xhtml_text_files(zf):\n",
    "            soup = BeautifulSoup(zf.read(file_name), 'lxml')\n",
    "            text = clean_text(soup.get_text('\\n'))\n",
    "            if len(text) < 800:\n",
    "                continue\n",
    "            title, pov = extract_title_and_pov(text)\n",
    "            chapters.append({\n",
    "                'chapter_id': len(chapters),\n",
    "                'epub_file': file_name,\n",
    "                'title': title,\n",
    "                'pov': pov,\n",
    "                'text': text,\n",
    "                'n_chars': len(text),\n",
    "            })\n",
    "    return pd.DataFrame(chapters)\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 4500, overlap: int = 750):\n",
    "    if chunk_size <= overlap:\n",
    "        raise ValueError('chunk_size debe ser mayor que overlap')\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        piece = text[start:end].strip()\n",
    "        if piece:\n",
    "            chunks.append((start, end, piece))\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunks(chapters_df: pd.DataFrame, chunk_size: int = 4500, overlap: int = 750) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, ch in chapters_df.iterrows():\n",
    "        for i, (s, e, t) in enumerate(chunk_text(ch['text'], chunk_size, overlap)):\n",
    "            rows.append({\n",
    "                'chunk_id': f\"{int(ch['chapter_id'])}_{i}\",\n",
    "                'chapter_id': int(ch['chapter_id']),\n",
    "                'epub_file': ch['epub_file'],\n",
    "                'title': ch['title'],\n",
    "                'pov': ch['pov'],\n",
    "                'start_char': s,\n",
    "                'end_char': e,\n",
    "                'text': t,\n",
    "                'n_chars': len(t),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1826170214.py:29: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  soup = BeautifulSoup(zf.read(file_name), 'lxml')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capítulos: 84 | Chunks: 509\n"
     ]
    }
   ],
   "source": [
    "chapters_df = extract_chapters(EPUB_PATH)\n",
    "chunks_df = build_chunks(chapters_df)\n",
    "\n",
    "chapters_df.to_parquet('chapters.parquet', index=False)\n",
    "chunks_df.to_parquet('chunks.parquet', index=False)\n",
    "\n",
    "print('Capítulos:', len(chapters_df), '| Chunks:', len(chunks_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4ba0b1fbaf41bd8bc2301b944e778e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0145107bc04a09997a2e2957b5ed20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2222a54a07a44a37838aa3d5570cb7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdbff53d27640ea9681d3927c780273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88e4623e34b4c53be212b424e035b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cec76512034ffa9e8dc3dcd867efcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f94839f905408ca7a6f0d92666d578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a073e8118e91486da6fe2a92932d0849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665ad6abfffa4f3fb3b74a43848021a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903e071f3927431086afc948bbfb57ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe83031c54d4e3587422870c3844669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25123cf218b24ce3aa30060ee8baa570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbc460bf69e497d922ff561aeca91d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad547e2eb62418ebccb853e8084b01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff5e1ecb73d43208dd4ffb031bd21bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acfa788ab0d4103a8e771f8dc866162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mXLMRobertaForSequenceClassification LOAD REPORT\u001b[0m from: BAAI/bge-reranker-large\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed9c1b023084381991da98df15199e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31310370a4fd4e8abb46ee73c1d61b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea35fca715d1433182aeda83a7d15f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95328f9bc174f90bd7318c46db103fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344291ee3ec148b880004091778c825c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2230e4b0180b4266acd135f06a7dd351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS listo: 509\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(EMBED_MODEL_ID)\n",
    "reranker = CrossEncoder(RERANKER_MODEL_ID)\n",
    "\n",
    "\n",
    "def embed_texts(texts, batch_size=32):\n",
    "    vecs = embedder.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    return vecs.astype('float32')\n",
    "\n",
    "emb = embed_texts(chunks_df['text'].tolist())\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "print('FAISS listo:', index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a538d2b8e6f4ed5a69f5802ffce962e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914817caeb264300a83b1776d3c32c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dec464bb35407090aad5a9df309416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0a00599fa446be980c58c503835db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d70a6de70a44f7bad9718702336531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7319d8eb47440ebf46b307ac0fd591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc0a8c6c97743cf82b4e5de4b4a87e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f5769813a1407a86ee162d42690661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e38d3c939149628ae4becd9e3d086f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b498be10c84a1b8a47f924f935edd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "\n",
    "def build_context(passages_df: pd.DataFrame, max_chars_each: int = 1800) -> str:\n",
    "    blocks = []\n",
    "    for _, row in passages_df.iterrows():\n",
    "        txt = row['text'][:max_chars_each].strip()\n",
    "        blocks.append(f\"[{row['chunk_id']}] ({row['pov']} | {row['title']})\\n{txt}\")\n",
    "    return '\\n\\n'.join(blocks)\n",
    "\n",
    "\n",
    "def run_chat(messages, max_new_tokens=400):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.03,\n",
    "    )\n",
    "    new_tokens = out[0][inputs['input_ids'].shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def retrieve_passages(question: str, top_k=12, faiss_k=100) -> pd.DataFrame:\n",
    "    q_emb = embed_texts([question], batch_size=1)\n",
    "    scores, idxs = index.search(q_emb, faiss_k)\n",
    "    cand = chunks_df.iloc[idxs[0].tolist()].copy()\n",
    "    cand['faiss_score'] = scores[0]\n",
    "    cand['rerank_score'] = reranker.predict([(question, t) for t in cand['text'].tolist()])\n",
    "    return cand.sort_values('rerank_score', ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def answer_question(question: str, passages_df: pd.DataFrame) -> str:\n",
    "    context = build_context(passages_df)\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                \"Eres experto en 'Juego de Tronos' (Libro 1). \"\n",
    "                \"Responde solo con hechos de los fragmentos. \"\n",
    "                \"Si no hay evidencia, di exactamente: 'No encontrado en los fragmentos proporcionados'. \"\n",
    "                \"Añade [chunk_id] al final de cada frase factual.\"\n",
    "            ),\n",
    "        },\n",
    "        {'role': 'user', 'content': f'Pregunta: {question}\\n\\nFragmentos:\\n{context}'},\n",
    "    ]\n",
    "    return run_chat(messages, max_new_tokens=420)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_schema = {\n",
    "    'style': 'string',\n",
    "    'subject': 'string',\n",
    "    'setting': 'string',\n",
    "    'time_of_day': 'day|night|dawn|dusk|unknown',\n",
    "    'mood': 'string',\n",
    "    'characters': [{'name': 'string', 'appearance': 'string', 'clothing': 'string'}],\n",
    "    'action': 'string',\n",
    "    'camera': 'string',\n",
    "    'important_objects': ['string'],\n",
    "    'avoid': ['string'],\n",
    "}\n",
    "\n",
    "\n",
    "def _balanced_json_substring(raw: str) -> str | None:\n",
    "    start = raw.find('{')\n",
    "    if start < 0:\n",
    "        return None\n",
    "\n",
    "    depth = 0\n",
    "    in_str = False\n",
    "    esc = False\n",
    "    for i in range(start, len(raw)):\n",
    "        ch = raw[i]\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == \"\\\\\":\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "                continue\n",
    "\n",
    "        if ch == '{':\n",
    "            depth += 1\n",
    "        elif ch == '}':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return raw[start:i+1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_first_json(raw: str) -> dict | None:\n",
    "\n",
    "    raw = (raw or '').strip()\n",
    "    raw = raw.replace('\\r', '').strip()\n",
    "    if not raw:\n",
    "        return None\n",
    "\n",
    "    # 1) Direct parse\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Remove markdown fences if present\n",
    "    cleaned = raw.replace('```json', '').replace('```JSON', '').replace('```', '').strip()\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Find balanced object in mixed text\n",
    "    sub = _balanced_json_substring(cleaned)\n",
    "    if sub:\n",
    "        try:\n",
    "            obj = json.loads(sub)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _scene_fallback(question: str, answer: str, passages_df: pd.DataFrame) -> dict:\n",
    "    # fallback robusto para no romper el flujo\n",
    "    top = passages_df.iloc[0] if len(passages_df) else None\n",
    "    setting = ''\n",
    "    subject = question\n",
    "    if top is not None:\n",
    "        pov = str(top.get('pov') or '').strip()\n",
    "        title = str(top.get('title') or '').strip()\n",
    "        setting = f\"{title} ({pov})\" if title or pov else 'Westeros medieval fantasy setting'\n",
    "\n",
    "    short_answer = re.sub(r'\\s+', ' ', (answer or '')).strip()[:220]\n",
    "    if not short_answer:\n",
    "        short_answer = 'scene based on retrieved chapter context'\n",
    "\n",
    "    return {\n",
    "        'style': 'cinematic realistic medieval fantasy',\n",
    "        'subject': subject,\n",
    "        'setting': setting or 'Westeros medieval fantasy setting',\n",
    "        'time_of_day': 'unknown',\n",
    "        'mood': 'dramatic',\n",
    "        'characters': [],\n",
    "        'action': short_answer,\n",
    "        'camera': 'medium shot, natural composition',\n",
    "        'important_objects': [],\n",
    "        'avoid': ['tv actors', 'celebrity face', 'modern clothing', 'text watermark logo'],\n",
    "    }\n",
    "\n",
    "\n",
    "def _normalize_scene(scene: dict, question: str, answer: str, passages_df: pd.DataFrame) -> dict:\n",
    "    base = _scene_fallback(question, answer, passages_df)\n",
    "    if not isinstance(scene, dict):\n",
    "        return base\n",
    "\n",
    "    for k in base:\n",
    "        if k not in scene or scene[k] in (None, ''):\n",
    "            scene[k] = base[k]\n",
    "\n",
    "    if not isinstance(scene.get('characters'), list):\n",
    "        scene['characters'] = []\n",
    "    if not isinstance(scene.get('important_objects'), list):\n",
    "        scene['important_objects'] = []\n",
    "    if not isinstance(scene.get('avoid'), list):\n",
    "        scene['avoid'] = base['avoid']\n",
    "\n",
    "    return scene\n",
    "\n",
    "\n",
    "def plan_scene(question: str, answer: str, passages_df: pd.DataFrame, debug: bool = False) -> dict:\n",
    "    context = build_context(passages_df, max_chars_each=1400)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                'You are an art director for text-to-image generation. '\n",
    "                'Return ONLY one valid JSON object (no markdown). '\n",
    "                'Use only grounded details from context and answer. '\n",
    "                'No actor names, no TV adaptation references.'\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                f'Question: {question}\\n\\nAnswer: {answer}\\n\\nContext:\\n{context}\\n\\n'\n",
    "                f'Schema: {json.dumps(scene_schema, ensure_ascii=False)}\\n'\n",
    "                'Output strictly a JSON object.'\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    raw = run_chat(messages, max_new_tokens=360)\n",
    "    scene = extract_first_json(raw)\n",
    "\n",
    "    # Retry: si el modelo dio texto, le pedimos convertirlo a JSON estricto\n",
    "    if scene is None:\n",
    "        fixer_messages = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'Convert the user content into exactly one valid JSON object. No extra text.',\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': (\n",
    "                    f'Schema: {json.dumps(scene_schema, ensure_ascii=False)}\\n\\n'\n",
    "                    f'Content to convert:\\n{raw}'\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "        fixed_raw = run_chat(fixer_messages, max_new_tokens=260)\n",
    "        scene = extract_first_json(fixed_raw)\n",
    "        if debug:\n",
    "            print('--- planner raw ---')\n",
    "            print(raw[:1200])\n",
    "            print('--- fixer raw ---')\n",
    "            print(fixed_raw[:1200])\n",
    "\n",
    "    # Fallback final: nunca romper ask_and_draw\n",
    "    scene = _normalize_scene(scene, question, answer, passages_df)\n",
    "    return scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "GatedRepoError",
     "evalue": "401 Client Error. (Request ID: Root=1-699854ac-6fdee0505c5d3f1d1641d4bd;ee983b75-864f-41df-a07a-9e548eeb787d)\n\nCannot access gated repo for url https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers/resolve/main/model_index.json.\nAccess to model stabilityai/stable-diffusion-3-medium-diffusers is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '401 Unauthorized' for url 'https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers/resolve/main/model_index.json'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-960531841.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m image_pipe = StableDiffusion3Pipeline.from_pretrained(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mSD3_MODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ).to('cuda')\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_attention_slicing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_legacy_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m                     \u001b[0;34m\" is neither a valid local path nor a valid repo id. Please check the parameter.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                 )\n\u001b[0;32m--> 829\u001b[0;31m             cached_folder = cls.download(\n\u001b[0m\u001b[1;32m    830\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_legacy_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(cls, pretrained_model_name, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m   1606\u001b[0m                 \u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_legacy_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         )\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1033\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;31m# If still error, raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhead_call_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m             \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;31m# From now on, etag, commit_hash, url and size are not None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder, retry_on_errors)\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1700\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_legacy_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, timeout, library_name, library_version, user_agent, headers, endpoint, retry_on_errors)\u001b[0m\n\u001b[1;32m   1620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m     response = _httpx_follow_relative_redirects(\n\u001b[0m\u001b[1;32m   1623\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_on_errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_httpx_follow_relative_redirects\u001b[0;34m(method, url, retry_on_errors, **httpx_kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mno_retry_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# Check if response is a relative redirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;34mf\"{response.status_code} Client Error.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"Cannot access gated repo for url {response.url}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             )\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0merror_message\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Access to this resource is disabled.\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-699854ac-6fdee0505c5d3f1d1641d4bd;ee983b75-864f-41df-a07a-9e548eeb787d)\n\nCannot access gated repo for url https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers/resolve/main/model_index.json.\nAccess to model stabilityai/stable-diffusion-3-medium-diffusers is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "image_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    SD3_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to('cuda')\n",
    "image_pipe.enable_attention_slicing()\n",
    "\n",
    "\n",
    "def ask_and_draw(question: str, top_k=12, faiss_k=100, seed: Optional[int] = None):\n",
    "    passages = retrieve_passages(question, top_k=top_k, faiss_k=faiss_k)\n",
    "    answer = answer_question(question, passages)\n",
    "    scene = plan_scene(question, answer, passages)\n",
    "    prompt, negative = scene_to_prompt(scene)\n",
    "\n",
    "    gen = None\n",
    "    if seed is not None:\n",
    "        gen = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "    image = image_pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=6.0,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        generator=gen,\n",
    "    ).images[0]\n",
    "\n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'passages': passages,\n",
    "        'scene': scene,\n",
    "        'prompt': prompt,\n",
    "        'negative_prompt': negative,\n",
    "        'image': image,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ask_and_draw('¿Cómo escapó Tyrion del Nido de Águilas?', seed=7)\n",
    "print(result['answer'])\n",
    "print('PROMPT:', result['prompt'])\n",
    "result['image']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
