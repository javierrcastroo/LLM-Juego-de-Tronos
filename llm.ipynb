{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juego de Tronos (Libro 1): QA + Imagen (Notebook independiente)\n",
    "\n",
    "Este notebook es **autocontenido**: no depende de archivos `.py` externos.\n",
    "Modelos:\n",
    "- QA/Planner: `Qwen/Qwen3-30B-A3B-Thinking-2507-FP8`\n",
    "- Imagen: `stabilityai/stable-diffusion-3.5-large`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U \"pandas==2.2.2\"\n",
    "!pip -q install -U pyarrow beautifulsoup4 lxml faiss-cpu sentence-transformers transformers accelerate diffusers safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "EPUB_PATH = '/content/drive/MyDrive/juego_de_tronos.epub'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from typing import Optional\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "QWEN_MODEL_ID = 'Qwen/Qwen3-30B-A3B-Thinking-2507'\n",
    "EMBED_MODEL_ID = 'BAAI/bge-m3'\n",
    "RERANKER_MODEL_ID = 'BAAI/bge-reranker-large'\n",
    "SD3_MODEL_ID = 'stabilityai/stable-diffusion-3-medium-diffusers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\r\\n?\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_title_and_pov(text: str) -> tuple[Optional[str], Optional[str]]:\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "    for line in lines[:20]:\n",
    "        if re.match(r\"^[A-ZÁÉÍÓÚÑÜ]+.*\\(\\d+\\)$\", line):\n",
    "            return line, line.split(\"(\")[0].strip()\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def list_xhtml_text_files(zf: zipfile.ZipFile) -> list[str]:\n",
    "    candidates = [f for f in zf.namelist() if f.lower().endswith((\".xhtml\", \".html\"))]\n",
    "    preferred = [f for f in candidates if '/text/' in f.lower() or '/texto/' in f.lower()]\n",
    "    return sorted(preferred if preferred else candidates)\n",
    "\n",
    "\n",
    "def extract_chapters(epub_path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(epub_path):\n",
    "        raise FileNotFoundError(f'No existe EPUB_PATH: {epub_path}')\n",
    "\n",
    "    chapters = []\n",
    "    with zipfile.ZipFile(epub_path, 'r') as zf:\n",
    "        for file_name in list_xhtml_text_files(zf):\n",
    "            soup = BeautifulSoup(zf.read(file_name), 'lxml')\n",
    "            text = clean_text(soup.get_text('\\n'))\n",
    "            if len(text) < 800:\n",
    "                continue\n",
    "            title, pov = extract_title_and_pov(text)\n",
    "            chapters.append({\n",
    "                'chapter_id': len(chapters),\n",
    "                'epub_file': file_name,\n",
    "                'title': title,\n",
    "                'pov': pov,\n",
    "                'text': text,\n",
    "                'n_chars': len(text),\n",
    "            })\n",
    "    return pd.DataFrame(chapters)\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 4500, overlap: int = 750):\n",
    "    if chunk_size <= overlap:\n",
    "        raise ValueError('chunk_size debe ser mayor que overlap')\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        piece = text[start:end].strip()\n",
    "        if piece:\n",
    "            chunks.append((start, end, piece))\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunks(chapters_df: pd.DataFrame, chunk_size: int = 4500, overlap: int = 750) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, ch in chapters_df.iterrows():\n",
    "        for i, (s, e, t) in enumerate(chunk_text(ch['text'], chunk_size, overlap)):\n",
    "            rows.append({\n",
    "                'chunk_id': f\"{int(ch['chapter_id'])}_{i}\",\n",
    "                'chapter_id': int(ch['chapter_id']),\n",
    "                'epub_file': ch['epub_file'],\n",
    "                'title': ch['title'],\n",
    "                'pov': ch['pov'],\n",
    "                'start_char': s,\n",
    "                'end_char': e,\n",
    "                'text': t,\n",
    "                'n_chars': len(t),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_df = extract_chapters(EPUB_PATH)\n",
    "chunks_df = build_chunks(chapters_df)\n",
    "\n",
    "chapters_df.to_parquet('chapters.parquet', index=False)\n",
    "chunks_df.to_parquet('chunks.parquet', index=False)\n",
    "\n",
    "print('Capítulos:', len(chapters_df), '| Chunks:', len(chunks_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(EMBED_MODEL_ID)\n",
    "reranker = CrossEncoder(RERANKER_MODEL_ID)\n",
    "\n",
    "\n",
    "def embed_texts(texts, batch_size=32):\n",
    "    vecs = embedder.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    return vecs.astype('float32')\n",
    "\n",
    "emb = embed_texts(chunks_df['text'].tolist())\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "print('FAISS listo:', index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "\n",
    "def build_context(passages_df: pd.DataFrame, max_chars_each: int = 1800) -> str:\n",
    "    blocks = []\n",
    "    for _, row in passages_df.iterrows():\n",
    "        txt = row['text'][:max_chars_each].strip()\n",
    "        blocks.append(f\"[{row['chunk_id']}] ({row['pov']} | {row['title']})\\n{txt}\")\n",
    "    return '\\n\\n'.join(blocks)\n",
    "\n",
    "\n",
    "def run_chat(messages, max_new_tokens=400):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.03,\n",
    "    )\n",
    "    new_tokens = out[0][inputs['input_ids'].shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def retrieve_passages(question: str, top_k=12, faiss_k=100) -> pd.DataFrame:\n",
    "    q_emb = embed_texts([question], batch_size=1)\n",
    "    scores, idxs = index.search(q_emb, faiss_k)\n",
    "    cand = chunks_df.iloc[idxs[0].tolist()].copy()\n",
    "    cand['faiss_score'] = scores[0]\n",
    "    cand['rerank_score'] = reranker.predict([(question, t) for t in cand['text'].tolist()])\n",
    "    return cand.sort_values('rerank_score', ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def answer_question(question: str, passages_df: pd.DataFrame) -> str:\n",
    "    context = build_context(passages_df)\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                \"Eres experto en 'Juego de Tronos' (Libro 1). \"\n",
    "                \"Responde solo con hechos de los fragmentos. \"\n",
    "                \"Si no hay evidencia, di exactamente: 'No encontrado en los fragmentos proporcionados'. \"\n",
    "                \"Añade [chunk_id] al final de cada frase factual.\"\n",
    "            ),\n",
    "        },\n",
    "        {'role': 'user', 'content': f'Pregunta: {question}\\n\\nFragmentos:\\n{context}'},\n",
    "    ]\n",
    "    return run_chat(messages, max_new_tokens=420)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_schema = {\n",
    "    'style': 'string',\n",
    "    'subject': 'string',\n",
    "    'setting': 'string',\n",
    "    'time_of_day': 'day|night|dawn|dusk|unknown',\n",
    "    'mood': 'string',\n",
    "    'characters': [{'name': 'string', 'appearance': 'string', 'clothing': 'string'}],\n",
    "    'action': 'string',\n",
    "    'camera': 'string',\n",
    "    'important_objects': ['string'],\n",
    "    'avoid': ['string'],\n",
    "}\n",
    "\n",
    "\n",
    "def _balanced_json_substring(raw: str) -> str | None:\n",
    "    start = raw.find('{')\n",
    "    if start < 0:\n",
    "        return None\n",
    "\n",
    "    depth = 0\n",
    "    in_str = False\n",
    "    esc = False\n",
    "    for i in range(start, len(raw)):\n",
    "        ch = raw[i]\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == '\\':\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "                continue\n",
    "\n",
    "        if ch == '{':\n",
    "            depth += 1\n",
    "        elif ch == '}':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return raw[start:i+1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_first_json(raw: str) -> dict | None:\n",
    "    raw = (raw or '').strip()\n",
    "    if not raw:\n",
    "        return None\n",
    "\n",
    "    # 1) Direct parse\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Remove markdown fences if present\n",
    "    cleaned = raw.replace('```json', '').replace('```JSON', '').replace('```', '').strip()\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Find balanced object in mixed text\n",
    "    sub = _balanced_json_substring(cleaned)\n",
    "    if sub:\n",
    "        try:\n",
    "            obj = json.loads(sub)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _scene_fallback(question: str, answer: str, passages_df: pd.DataFrame) -> dict:\n",
    "    # fallback robusto para no romper el flujo\n",
    "    top = passages_df.iloc[0] if len(passages_df) else None\n",
    "    setting = ''\n",
    "    subject = question\n",
    "    if top is not None:\n",
    "        pov = str(top.get('pov') or '').strip()\n",
    "        title = str(top.get('title') or '').strip()\n",
    "        setting = f\"{title} ({pov})\" if title or pov else 'Westeros medieval fantasy setting'\n",
    "\n",
    "    short_answer = re.sub(r'\\s+', ' ', (answer or '')).strip()[:220]\n",
    "    if not short_answer:\n",
    "        short_answer = 'scene based on retrieved chapter context'\n",
    "\n",
    "    return {\n",
    "        'style': 'cinematic realistic medieval fantasy',\n",
    "        'subject': subject,\n",
    "        'setting': setting or 'Westeros medieval fantasy setting',\n",
    "        'time_of_day': 'unknown',\n",
    "        'mood': 'dramatic',\n",
    "        'characters': [],\n",
    "        'action': short_answer,\n",
    "        'camera': 'medium shot, natural composition',\n",
    "        'important_objects': [],\n",
    "        'avoid': ['tv actors', 'celebrity face', 'modern clothing', 'text watermark logo'],\n",
    "    }\n",
    "\n",
    "\n",
    "def _normalize_scene(scene: dict, question: str, answer: str, passages_df: pd.DataFrame) -> dict:\n",
    "    base = _scene_fallback(question, answer, passages_df)\n",
    "    if not isinstance(scene, dict):\n",
    "        return base\n",
    "\n",
    "    for k in base:\n",
    "        if k not in scene or scene[k] in (None, ''):\n",
    "            scene[k] = base[k]\n",
    "\n",
    "    if not isinstance(scene.get('characters'), list):\n",
    "        scene['characters'] = []\n",
    "    if not isinstance(scene.get('important_objects'), list):\n",
    "        scene['important_objects'] = []\n",
    "    if not isinstance(scene.get('avoid'), list):\n",
    "        scene['avoid'] = base['avoid']\n",
    "\n",
    "    return scene\n",
    "\n",
    "\n",
    "def plan_scene(question: str, answer: str, passages_df: pd.DataFrame, debug: bool = False) -> dict:\n",
    "    context = build_context(passages_df, max_chars_each=1400)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                'You are an art director for text-to-image generation. '\n",
    "                'Return ONLY one valid JSON object (no markdown). '\n",
    "                'Use only grounded details from context and answer. '\n",
    "                'No actor names, no TV adaptation references.'\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                f'Question: {question}\\n\\nAnswer: {answer}\\n\\nContext:\\n{context}\\n\\n'\n",
    "                f'Schema: {json.dumps(scene_schema, ensure_ascii=False)}\\n'\n",
    "                'Output strictly a JSON object.'\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    raw = run_chat(messages, max_new_tokens=360)\n",
    "    scene = extract_first_json(raw)\n",
    "\n",
    "    # Retry: si el modelo dio texto, le pedimos convertirlo a JSON estricto\n",
    "    if scene is None:\n",
    "        fixer_messages = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'Convert the user content into exactly one valid JSON object. No extra text.',\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': (\n",
    "                    f'Schema: {json.dumps(scene_schema, ensure_ascii=False)}\\n\\n'\n",
    "                    f'Content to convert:\\n{raw}'\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "        fixed_raw = run_chat(fixer_messages, max_new_tokens=260)\n",
    "        scene = extract_first_json(fixed_raw)\n",
    "        if debug:\n",
    "            print('--- planner raw ---')\n",
    "            print(raw[:1200])\n",
    "            print('--- fixer raw ---')\n",
    "            print(fixed_raw[:1200])\n",
    "\n",
    "    # Fallback final: nunca romper ask_and_draw\n",
    "    scene = _normalize_scene(scene, question, answer, passages_df)\n",
    "    return scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    SD3_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to('cuda')\n",
    "image_pipe.enable_attention_slicing()\n",
    "\n",
    "\n",
    "def ask_and_draw(question: str, top_k=12, faiss_k=100, seed: Optional[int] = None):\n",
    "    passages = retrieve_passages(question, top_k=top_k, faiss_k=faiss_k)\n",
    "    answer = answer_question(question, passages)\n",
    "    scene = plan_scene(question, answer, passages)\n",
    "    prompt, negative = scene_to_prompt(scene)\n",
    "\n",
    "    gen = None\n",
    "    if seed is not None:\n",
    "        gen = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "    image = image_pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=6.0,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        generator=gen,\n",
    "    ).images[0]\n",
    "\n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'passages': passages,\n",
    "        'scene': scene,\n",
    "        'prompt': prompt,\n",
    "        'negative_prompt': negative,\n",
    "        'image': image,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ask_and_draw('¿Cómo escapó Tyrion del Nido de Águilas?', seed=7)\n",
    "print(result['answer'])\n",
    "print('PROMPT:', result['prompt'])\n",
    "result['image']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
