{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juego de Tronos Libro 1 — RAG extremo + Generación de Imagen (Notebook autocontenido)\n",
    "\n",
    "Este notebook está pensado para exprimir calidad al máximo:\n",
    "- Retrieval híbrido (dense + BM25) con MMR.\n",
    "- Expansión multi-consulta opcional.\n",
    "- QA grounded con verificación/filtro de fidelidad.\n",
    "- Planner de escena robusto + prompt visual avanzado.\n",
    "- Generación multi-seed y selección automática por score semántico con CLIP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U pandas pyarrow beautifulsoup4 lxml faiss-cpu sentence-transformers transformers accelerate diffusers safetensors rank-bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "EPUB_PATH = '/content/drive/MyDrive/juego_de_tronos.epub'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from typing import Optional\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "QWEN_MODEL_ID = 'Qwen/Qwen3-30B-A3B-Thinking-2507-FP8'\n",
    "EMBED_MODEL_ID = 'BAAI/bge-m3'\n",
    "RERANKER_MODEL_ID = 'BAAI/bge-reranker-large'\n",
    "SD3_MODEL_ID = 'stabilityai/stable-diffusion-3.5-large'\n",
    "CLIP_MODEL_ID = 'openai/clip-vit-large-patch14'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\r\\n?\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_title_and_pov(text: str) -> tuple[Optional[str], Optional[str]]:\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "    for line in lines[:20]:\n",
    "        if re.match(r\"^[A-ZÁÉÍÓÚÑÜ]+.*\\(\\d+\\)$\", line):\n",
    "            return line, line.split(\"(\")[0].strip()\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def list_xhtml_text_files(zf: zipfile.ZipFile) -> list[str]:\n",
    "    candidates = [f for f in zf.namelist() if f.lower().endswith((\".xhtml\", \".html\"))]\n",
    "    preferred = [f for f in candidates if '/text/' in f.lower() or '/texto/' in f.lower()]\n",
    "    return sorted(preferred if preferred else candidates)\n",
    "\n",
    "\n",
    "def extract_chapters(epub_path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(epub_path):\n",
    "        raise FileNotFoundError(f'No existe EPUB_PATH: {epub_path}')\n",
    "\n",
    "    chapters = []\n",
    "    with zipfile.ZipFile(epub_path, 'r') as zf:\n",
    "        for file_name in list_xhtml_text_files(zf):\n",
    "            soup = BeautifulSoup(zf.read(file_name), 'lxml')\n",
    "            text = clean_text(soup.get_text('\\n'))\n",
    "            if len(text) < 800:\n",
    "                continue\n",
    "            title, pov = extract_title_and_pov(text)\n",
    "            chapters.append({\n",
    "                'chapter_id': len(chapters),\n",
    "                'epub_file': file_name,\n",
    "                'title': title,\n",
    "                'pov': pov,\n",
    "                'text': text,\n",
    "                'n_chars': len(text),\n",
    "            })\n",
    "    return pd.DataFrame(chapters)\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 3500, overlap: int = 500):\n",
    "    if chunk_size <= overlap:\n",
    "        raise ValueError('chunk_size debe ser mayor que overlap')\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        piece = text[start:end].strip()\n",
    "        if piece:\n",
    "            chunks.append((start, end, piece))\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunks(chapters_df: pd.DataFrame, chunk_size: int = 3500, overlap: int = 500) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, ch in chapters_df.iterrows():\n",
    "        for i, (s, e, t) in enumerate(chunk_text(ch['text'], chunk_size, overlap)):\n",
    "            rows.append({\n",
    "                'chunk_id': f\"{int(ch['chapter_id'])}_{i}\",\n",
    "                'chapter_id': int(ch['chapter_id']),\n",
    "                'epub_file': ch['epub_file'],\n",
    "                'title': ch['title'],\n",
    "                'pov': ch['pov'],\n",
    "                'start_char': s,\n",
    "                'end_char': e,\n",
    "                'text': t,\n",
    "                'n_chars': len(t),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_df = extract_chapters(EPUB_PATH)\n",
    "chunks_df = build_chunks(chapters_df)\n",
    "\n",
    "chapters_df.to_parquet('chapters.parquet', index=False)\n",
    "chunks_df.to_parquet('chunks.parquet', index=False)\n",
    "print('Capítulos:', len(chapters_df), '| Chunks:', len(chunks_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(EMBED_MODEL_ID)\n",
    "reranker = CrossEncoder(RERANKER_MODEL_ID)\n",
    "\n",
    "\n",
    "def embed_texts(texts, batch_size=32):\n",
    "    vecs = embedder.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    return vecs.astype('float32')\n",
    "\n",
    "# Dense index\n",
    "emb = embed_texts(chunks_df['text'].tolist())\n",
    "faiss_index = faiss.IndexFlatIP(emb.shape[1])\n",
    "faiss_index.add(emb)\n",
    "\n",
    "# Lexical index (BM25)\n",
    "def bm25_tokenize(text: str):\n",
    "    return re.findall(r\"\\w+\", text.lower(), flags=re.UNICODE)\n",
    "\n",
    "bm25_corpus = [bm25_tokenize(t) for t in chunks_df['text'].tolist()]\n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "\n",
    "print('FAISS:', faiss_index.ntotal, '| BM25 docs:', len(bm25_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "\n",
    "def run_chat(messages, max_new_tokens=400, do_sample=False, temperature=0.0):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.03,\n",
    "    )\n",
    "    new_tokens = out[0][inputs['input_ids'].shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(question: str, enable=True, n=3):\n",
    "    if not enable:\n",
    "        return [question]\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'Genera variantes cortas de búsqueda para recuperar pasajes de novela. Devuelve JSON con clave queries.',\n",
    "        },\n",
    "        {'role': 'user', 'content': f'Pregunta: {question} | n={n}'},\n",
    "    ]\n",
    "    raw = run_chat(messages, max_new_tokens=180)\n",
    "    try:\n",
    "        js = json.loads(raw[raw.find('{'):raw.rfind('}')+1])\n",
    "        q = [question] + [x.strip() for x in js.get('queries', []) if isinstance(x, str)]\n",
    "        return list(dict.fromkeys([x for x in q if x]))[:n+1]\n",
    "    except Exception:\n",
    "        return [question]\n",
    "\n",
    "\n",
    "def minmax_norm(arr):\n",
    "    arr = np.array(arr, dtype=np.float32)\n",
    "    if len(arr) == 0:\n",
    "        return arr\n",
    "    mn, mx = float(arr.min()), float(arr.max())\n",
    "    if mx - mn < 1e-8:\n",
    "        return np.zeros_like(arr)\n",
    "    return (arr - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "def mmr_select(candidates_df: pd.DataFrame, query_emb: np.ndarray, top_k=12, lambda_mult=0.75):\n",
    "    if len(candidates_df) <= top_k:\n",
    "        return candidates_df.reset_index(drop=True)\n",
    "\n",
    "    cand_emb = np.vstack(candidates_df['dense_vec'].tolist())\n",
    "    sim_query = cand_emb @ query_emb\n",
    "\n",
    "    selected = []\n",
    "    remaining = list(range(len(candidates_df)))\n",
    "\n",
    "    while remaining and len(selected) < top_k:\n",
    "        if not selected:\n",
    "            best = max(remaining, key=lambda i: sim_query[i])\n",
    "            selected.append(best)\n",
    "            remaining.remove(best)\n",
    "            continue\n",
    "\n",
    "        def mmr_score(i):\n",
    "            redundancy = max(float(cand_emb[i] @ cand_emb[j]) for j in selected)\n",
    "            return lambda_mult * float(sim_query[i]) - (1 - lambda_mult) * redundancy\n",
    "\n",
    "        best = max(remaining, key=mmr_score)\n",
    "        selected.append(best)\n",
    "        remaining.remove(best)\n",
    "\n",
    "    return candidates_df.iloc[selected].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def hybrid_retrieve(question: str, top_k=12, faiss_k=120, bm25_k=120, alpha=0.6, use_query_expansion=True):\n",
    "    queries = expand_query(question, enable=use_query_expansion, n=3)\n",
    "\n",
    "    dense_scores = {}\n",
    "    bm25_scores = {}\n",
    "    query_emb_main = embed_texts([question], batch_size=1)[0]\n",
    "\n",
    "    for q in queries:\n",
    "        # Dense\n",
    "        q_emb = embed_texts([q], batch_size=1)\n",
    "        scores, idxs = faiss_index.search(q_emb, faiss_k)\n",
    "        for s, idx in zip(scores[0].tolist(), idxs[0].tolist()):\n",
    "            dense_scores[idx] = max(dense_scores.get(idx, -1e9), float(s))\n",
    "\n",
    "        # Lexical\n",
    "        bm = bm25.get_scores(bm25_tokenize(q))\n",
    "        top_idx = np.argsort(bm)[::-1][:bm25_k]\n",
    "        for idx in top_idx:\n",
    "            bm25_scores[int(idx)] = max(bm25_scores.get(int(idx), -1e9), float(bm[idx]))\n",
    "\n",
    "    union = sorted(set(dense_scores.keys()) | set(bm25_scores.keys()))\n",
    "    if not union:\n",
    "        return chunks_df.head(top_k).copy()\n",
    "\n",
    "    d = minmax_norm([dense_scores.get(i, min(dense_scores.values()) if dense_scores else 0.0) for i in union])\n",
    "    b = minmax_norm([bm25_scores.get(i, min(bm25_scores.values()) if bm25_scores else 0.0) for i in union])\n",
    "    h = alpha * d + (1 - alpha) * b\n",
    "\n",
    "    cand = chunks_df.iloc[union].copy().reset_index(drop=True)\n",
    "    cand['dense_score'] = d\n",
    "    cand['bm25_score'] = b\n",
    "    cand['hybrid_score'] = h\n",
    "    cand['dense_vec'] = [emb[i] for i in union]\n",
    "\n",
    "    # Rerank\n",
    "    pairs = [(question, t) for t in cand['text'].tolist()]\n",
    "    cand['rerank_score'] = reranker.predict(pairs)\n",
    "    cand = cand.sort_values(['rerank_score', 'hybrid_score'], ascending=False).head(max(top_k * 3, 24)).reset_index(drop=True)\n",
    "\n",
    "    # MMR diversify\n",
    "    cand = mmr_select(cand, query_emb_main, top_k=top_k, lambda_mult=0.75)\n",
    "\n",
    "    return cand.drop(columns=['dense_vec'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(passages_df: pd.DataFrame, max_chars_each: int = 1500) -> str:\n",
    "    blocks = []\n",
    "    for _, row in passages_df.iterrows():\n",
    "        txt = row['text'][:max_chars_each].strip()\n",
    "        blocks.append(f\"[{row['chunk_id']}] ({row['pov']} | {row['title']})\\n{txt}\")\n",
    "    return '\\n\\n'.join(blocks)\n",
    "\n",
    "\n",
    "def answer_question(question: str, passages_df: pd.DataFrame) -> str:\n",
    "    context = build_context(passages_df)\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                \"Eres experto en 'Juego de Tronos' (Libro 1). \"\n",
    "                \"Responde únicamente con hechos sustentados en fragmentos. \"\n",
    "                \"Formato obligatorio: 1) respuesta breve y clara, 2) viñetas con evidencia textual, \"\n",
    "                \"3) referencias [chunk_id] por afirmación. \"\n",
    "                \"Si no hay evidencia suficiente: 'No encontrado en los fragmentos proporcionados'.\"\n",
    "            ),\n",
    "        },\n",
    "        {'role': 'user', 'content': f'Pregunta: {question}\\n\\nFragmentos:\\n{context}'},\n",
    "    ]\n",
    "    return run_chat(messages, max_new_tokens=520)\n",
    "\n",
    "\n",
    "def verify_answer(question: str, answer: str, passages_df: pd.DataFrame):\n",
    "    context = build_context(passages_df, max_chars_each=1200)\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'Evalúa fidelidad factual. Devuelve SOLO JSON: {\"faithful\": bool, \"issues\": [str], \"rewrite\": str}',\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Pregunta: {question}\\n\\nRespuesta: {answer}\\n\\nContexto:\\n{context}',\n",
    "        },\n",
    "    ]\n",
    "    raw = run_chat(messages, max_new_tokens=260)\n",
    "    try:\n",
    "        js = json.loads(raw[raw.find('{'):raw.rfind('}')+1])\n",
    "        faithful = bool(js.get('faithful', False))\n",
    "        rewrite = js.get('rewrite', answer)\n",
    "        issues = js.get('issues', [])\n",
    "        if not faithful and isinstance(rewrite, str) and rewrite.strip():\n",
    "            return rewrite.strip(), issues\n",
    "        return answer, issues\n",
    "    except Exception:\n",
    "        return answer, ['No se pudo verificar automáticamente']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_schema = {\n",
    "    'style': 'string',\n",
    "    'subject': 'string',\n",
    "    'setting': 'string',\n",
    "    'time_of_day': 'day|night|dawn|dusk|unknown',\n",
    "    'weather': 'string',\n",
    "    'mood': 'string',\n",
    "    'characters': [{'name': 'string', 'appearance': 'string', 'clothing': 'string'}],\n",
    "    'action': 'string',\n",
    "    'camera': 'string',\n",
    "    'palette': 'string',\n",
    "    'important_objects': ['string'],\n",
    "    'avoid': ['string'],\n",
    "}\n",
    "def _balanced_json_substring(raw: str) -> str | None:\n",
    "    start = raw.find('{')\n",
    "    if start < 0:\n",
    "        return None\n",
    "    depth, in_str, esc = 0, False, False\n",
    "    for i in range(start, len(raw)):\n",
    "        ch = raw[i]\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == '\\\\':\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "        if ch == '\"':\n",
    "            in_str = True\n",
    "            continue\n",
    "        if ch == '{':\n",
    "            depth += 1\n",
    "        elif ch == '}':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return raw[start:i+1]\n",
    "    return None\n",
    "def extract_first_json(raw: str) -> dict | None:\n",
    "    raw = (raw or '').strip()\n",
    "    if not raw:\n",
    "        return None\n",
    "    for candidate in [raw, raw.replace('```json', '').replace('```JSON', '').replace('```', '').strip()]:\n",
    "        try:\n",
    "            obj = json.loads(candidate)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "        sub = _balanced_json_substring(candidate)\n",
    "        if sub:\n",
    "            try:\n",
    "                obj = json.loads(sub)\n",
    "                if isinstance(obj, dict):\n",
    "                    return obj\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "def _scene_fallback(question: str, answer: str, passages_df: pd.DataFrame) -> dict:\n",
    "    top = passages_df.iloc[0] if len(passages_df) else None\n",
    "    loc = 'Westeros medieval fantasy setting'\n",
    "    if top is not None:\n",
    "        t = str(top.get('title') or '').strip()\n",
    "        p = str(top.get('pov') or '').strip()\n",
    "        if t or p:\n",
    "            loc = f'{t} ({p})'\n",
    "    return {\n",
    "        'style': 'cinematic realism, epic medieval fantasy',\n",
    "        'subject': question,\n",
    "        'setting': loc,\n",
    "        'time_of_day': 'unknown',\n",
    "        'weather': 'moody atmosphere',\n",
    "        'mood': 'dramatic',\n",
    "        'characters': [],\n",
    "        'action': re.sub(r'\\s+', ' ', answer)[:220],\n",
    "        'camera': 'dynamic medium shot, 35mm lens',\n",
    "        'palette': 'cold desaturated tones',\n",
    "        'important_objects': [],\n",
    "        'avoid': ['tv actors', 'celebrity likeness', 'modern clothing', 'watermark', 'text'],\n",
    "    }\n",
    "def normalize_scene(scene: dict, question: str, answer: str, passages_df: pd.DataFrame) -> dict:\n",
    "    base = _scene_fallback(question, answer, passages_df)\n",
    "    if not isinstance(scene, dict):\n",
    "        return base\n",
    "    for k, v in base.items():\n",
    "        if k not in scene or scene[k] in (None, ''):\n",
    "            scene[k] = v\n",
    "    for k in ['characters', 'important_objects', 'avoid']:\n",
    "        if not isinstance(scene.get(k), list):\n",
    "            scene[k] = base[k]\n",
    "    return scene\n",
    "def plan_scene(question: str, answer: str, passages_df: pd.DataFrame, debug=False) -> dict:\n",
    "    context = build_context(passages_df, max_chars_each=1200)\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                'You are a cinematic art director. Return ONLY one valid JSON object. '\n",
    "                'Ground details in context. No TV actor names.'\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                f'Question: {question}\\n\\nAnswer: {answer}\\n\\nContext:\\n{context}\\n\\n'\n",
    "                f'Schema: {json.dumps(scene_schema, ensure_ascii=False)}\\n'\n",
    "                'Output strictly JSON only.'\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    raw = run_chat(messages, max_new_tokens=420)\n",
    "    scene = extract_first_json(raw)\n",
    "    if scene is None:\n",
    "        fixer = [\n",
    "            {'role': 'system', 'content': 'Convert content into exactly one valid JSON object.'},\n",
    "            {'role': 'user', 'content': f'Schema: {json.dumps(scene_schema, ensure_ascii=False)}\\n\\nContent:\\n{raw}'},\n",
    "        ]\n",
    "        raw_fix = run_chat(fixer, max_new_tokens=260)\n",
    "        scene = extract_first_json(raw_fix)\n",
    "        if debug:\n",
    "            print(raw[:1000])\n",
    "            print(raw_fix[:1000])\n",
    "    return normalize_scene(scene, question, answer, passages_df)\n",
    "def scene_to_prompt(scene: dict) -> tuple[str, str]:\n",
    "    cinematic_booster = 'masterpiece, cinematic still, ultra detailed, volumetric light, film grain, sharp focus'\n",
    "    chars = []\n",
    "    for c in (scene.get('characters') or [])[:4]:\n",
    "        desc = ', '.join([x for x in [c.get('name'), c.get('appearance'), c.get('clothing')] if x])\n",
    "        if desc:\n",
    "            chars.append(desc)\n",
    "    parts = [\n",
    "        cinematic_booster,\n",
    "        scene.get('style', ''),\n",
    "        f\"subject: {scene.get('subject', '')}\",\n",
    "        scene.get('action', ''),\n",
    "        f\"setting: {scene.get('setting', '')}\",\n",
    "        f\"time: {scene.get('time_of_day', '')}\",\n",
    "        f\"weather: {scene.get('weather', '')}\",\n",
    "        f\"mood: {scene.get('mood', '')}\",\n",
    "        f\"palette: {scene.get('palette', '')}\",\n",
    "        f\"characters: {'; '.join(chars)}\" if chars else '',\n",
    "        f\"camera: {scene.get('camera', '')}\",\n",
    "        'props: ' + ', '.join(scene.get('important_objects', [])) if scene.get('important_objects') else '',\n",
    "    ]\n",
    "    prompt = ', '.join([p.strip() for p in parts if p and str(p).strip()])\n",
    "    negative = (scene.get('avoid') or []) + [\n",
    "        'worst quality, low quality, blurry, jpeg artifacts',\n",
    "        'text, watermark, logo, subtitles',\n",
    "        'modern objects, modern clothes, cars, smartphones',\n",
    "        'tv actors, celebrity face',\n",
    "        'extra fingers, malformed hands, bad anatomy',\n",
    "    ]\n",
    "    return prompt, ', '.join(dict.fromkeys(negative))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    SD3_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to('cuda')\n",
    "image_pipe.enable_attention_slicing()\n",
    "\n",
    "# CLIP para elegir mejor imagen entre varias seeds\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID).to('cuda')\n",
    "clip_proc = CLIPProcessor.from_pretrained(CLIP_MODEL_ID)\n",
    "\n",
    "\n",
    "def clip_text_image_score(prompt: str, image):\n",
    "    inputs = clip_proc(text=[prompt], images=image, return_tensors='pt', padding=True).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        text_emb = outputs.text_embeds / outputs.text_embeds.norm(dim=-1, keepdim=True)\n",
    "        image_emb = outputs.image_embeds / outputs.image_embeds.norm(dim=-1, keepdim=True)\n",
    "        score = (text_emb * image_emb).sum().item()\n",
    "    return score\n",
    "\n",
    "\n",
    "def generate_best_image(prompt: str, negative: str, seeds=(7, 13, 23), width=1024, height=1024, steps=32, guidance=6.5):\n",
    "    candidates = []\n",
    "    for s in seeds:\n",
    "        gen = torch.Generator(device='cuda').manual_seed(int(s))\n",
    "        img = image_pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=guidance,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            generator=gen,\n",
    "        ).images[0]\n",
    "        score = clip_text_image_score(prompt, img)\n",
    "        candidates.append((score, s, img))\n",
    "\n",
    "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "    best_score, best_seed, best_img = candidates[0]\n",
    "    return best_img, {'best_seed': best_seed, 'best_clip_score': best_score, 'all_scores': [(s, float(sc)) for sc, s, _ in candidates]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_draw_insane(question: str, top_k=14, faiss_k=140, bm25_k=140, alpha=0.6, query_expansion=True, debug=False):\n",
    "    passages = hybrid_retrieve(\n",
    "        question,\n",
    "        top_k=top_k,\n",
    "        faiss_k=faiss_k,\n",
    "        bm25_k=bm25_k,\n",
    "        alpha=alpha,\n",
    "        use_query_expansion=query_expansion,\n",
    "    )\n",
    "\n",
    "    answer_raw = answer_question(question, passages)\n",
    "    answer_final, faithfulness_issues = verify_answer(question, answer_raw, passages)\n",
    "\n",
    "    scene = plan_scene(question, answer_final, passages, debug=debug)\n",
    "    prompt, negative = scene_to_prompt(scene)\n",
    "\n",
    "    image, image_meta = generate_best_image(prompt, negative, seeds=(7, 17, 29, 43), steps=34, guidance=6.5)\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer_raw': answer_raw,\n",
    "        'answer_final': answer_final,\n",
    "        'faithfulness_issues': faithfulness_issues,\n",
    "        'passages': passages,\n",
    "        'scene': scene,\n",
    "        'prompt': prompt,\n",
    "        'negative_prompt': negative,\n",
    "        'image_meta': image_meta,\n",
    "        'image': image,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ask_and_draw_insane('¿Cómo escapó Tyrion del Nido de Águilas?', debug=True)\n",
    "print('RESPUESTA FINAL:\\n', result['answer_final'])\n",
    "print('\\nIssues fidelidad:', result['faithfulness_issues'])\n",
    "print('\\nPrompt final:\\n', result['prompt'])\n",
    "print('\\nImage meta:', result['image_meta'])\n",
    "result['image']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}